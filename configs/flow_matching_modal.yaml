data:
  dataset: "celeba"
  root: "/data/celeba"
  from_hub: true
  repo_name: "electronickale/cmu-10799-celeba64-subset"
  image_size: 64
  channels: 3
  num_workers: 4
  pin_memory: true
  augment: true

model:
  base_channels: 64
  channel_mult: [1, 2, 2, 4]
  num_res_blocks: 2
  attention_resolutions: [16, 8]
  num_heads: 4
  dropout: 0.1
  use_scale_shift_norm: true

training:
  batch_size: 128
  learning_rate: 0.0002
  weight_decay: 0.00
  betas: [0.9, 0.999]
  ema_decay: 0.9999
  ema_start: 2000
  gradient_clip_norm: 1.0 # Important: Keep this for Flow Matching stability!
  num_iterations: 120000
  log_every: 100
  sample_every: 5000
  save_every: 10000
  num_samples: 16

sampling:
  num_steps: 100  # Flow Matching usually requires fewer steps
  sampler: "euler" 

infrastructure:
  seed: 42
  device: "cuda"
  num_gpus: 1
  mixed_precision: true
  compile_model: false

checkpoint:
  dir: "./checkpoints"
  resume: False # Set to path (e.g., "./logs/.../checkpoints/") 

logging:
  dir: "./logs"
  wandb:
    enabled: true
    project: "cmu-10799-hw2-fm" # Suggest using a new project or tag for HW2
    entity: null
    # resume_id: null # Will be auto-filled by train.py if resuming from a directory